{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/IqP0gzAxjhZzlXf66J97",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrm003/cc-lab04/blob/main/lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3oj7uxARqHG",
        "outputId": "44cc38da-8231-4ddf-d952-16f4e1da72fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=a64f6760e67b7fa30066174dcbbd783db609f29662f45f53ea10409b8e300ea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL1cH8XEQZbA"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating spark session\n",
        "session = SparkSession.builder.master(\"local\").appName(\"Test\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./words.txt\"\n",
        "df = session.read.text(path)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcG287OESIDb",
        "outputId": "46d1a97e-3e3e-44b5-b590-77442b021f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|    value|\n",
            "+---------+\n",
            "|        A|\n",
            "|        a|\n",
            "|       aa|\n",
            "|      aal|\n",
            "|    aalii|\n",
            "|      aam|\n",
            "|     Aani|\n",
            "| aardvark|\n",
            "| aardwolf|\n",
            "|    Aaron|\n",
            "|  Aaronic|\n",
            "|Aaronical|\n",
            "| Aaronite|\n",
            "|Aaronitic|\n",
            "|     Aaru|\n",
            "|       Ab|\n",
            "|      aba|\n",
            "|  Ababdeh|\n",
            "|   Ababua|\n",
            "|     abac|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 List the 5 first words (in ascending order) and from the words.txt which start with “b” and end with “t”.\n",
        "\n",
        "# filter data to get strings starting from b and endind with t\n",
        "dfwithfilter = df.filter(df.value.like('b%t'))\n",
        "dfwithfilter.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5W8gy-oSong",
        "outputId": "195d413c-4692-4913-c0ca-10dc899ff90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|     value|\n",
            "+----------+\n",
            "|   babbitt|\n",
            "|babblement|\n",
            "|   babelet|\n",
            "|baboonroot|\n",
            "|    baboot|\n",
            "|   babroot|\n",
            "|  baccarat|\n",
            "|  bacchant|\n",
            "|  backcast|\n",
            "|  backchat|\n",
            "| backcourt|\n",
            "|    backet|\n",
            "| backjoint|\n",
            "|   backlet|\n",
            "|  backmost|\n",
            "|   backset|\n",
            "| backshift|\n",
            "| backsight|\n",
            "| backswept|\n",
            "|  backwort|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# order in ascending order and get first 5.\n",
        "dfwithfilter.orderBy(df.value).limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBRDBEgCSsQ0",
        "outputId": "4f17409c-040d-4a58-8dda-5a63477bae53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|     value|\n",
            "+----------+\n",
            "|   babbitt|\n",
            "|babblement|\n",
            "|   babelet|\n",
            "|baboonroot|\n",
            "|    baboot|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 List the 10 last longest words from the file words.txt.\n",
        "\n",
        "from pyspark.sql.functions import col,length,trim"
      ],
      "metadata": {
        "id": "CfVcqZjcSxiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# order the string data in descending order of lenght and filter top 10 records.\n",
        "df.orderBy(length(col(\"value\")).desc()).limit(10).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3luTSYsQS0Ku",
        "outputId": "66a13bea-7c20-4cd2-e669-18aa1d6d0001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|formaldehydesulph...|\n",
            "|pathologicopsycho...|\n",
            "|scientificophilos...|\n",
            "|tetraiodophenolph...|\n",
            "|thyroparathyroide...|\n",
            "|anthropomorpholog...|\n",
            "|hematospectrophot...|\n",
            "|macracanthrorhync...|\n",
            "|pericardiomediast...|\n",
            "|pancreaticoduoden...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3 Calculate the number of lines and the number of distinct words from file1.\n",
        "f1path = './file1.txt'\n",
        "dff1 = session.read.text(f1path)\n",
        "\n",
        "#  number of lines\n",
        "dff1.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCZCVvscS4sQ",
        "outputId": "b1d03778-1cfd-486f-c441-d63677af776c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "372624"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of distinct words\n",
        "dfDstinct = df.distinct()"
      ],
      "metadata": {
        "id": "ECgR-o4SS8l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfDstinct.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5gqs9_jTPAk",
        "outputId": "caeac209-eacd-4a01-de10-592b689d0f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|         value|\n",
            "+--------------+\n",
            "|      abeltree|\n",
            "|         Abner|\n",
            "|   abnormalize|\n",
            "|     abnormous|\n",
            "|   Abrahamitic|\n",
            "|    abruptness|\n",
            "|     acanthoma|\n",
            "|  Acanthopanax|\n",
            "|   Acanthophis|\n",
            "|      accensor|\n",
            "|  accumulation|\n",
            "|       acidity|\n",
            "|        acnode|\n",
            "|   acromegalia|\n",
            "|   adaptionism|\n",
            "|   additionary|\n",
            "|      adjudger|\n",
            "|      Aesopian|\n",
            "|afflictionless|\n",
            "|      affusion|\n",
            "+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfDstinct.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjBxwwFMTRn-",
        "outputId": "93af48a0-ab23-461b-ec01-061f9d5cf968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235886"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.4 Find 3 common words in files1, file2, and file3 which their sum of frequencies (number of occurrences)\n",
        "# within the three files is maximum compared to the other shred words. (e.g., assume that X is a set of words which\n",
        "# are shared between all files, find common words like xi in X which maximize\n",
        "# F(xi)= f1(count xi in file 1) +f2(count xi in file 2) +f3(count xi in file 3)."
      ],
      "metadata": {
        "id": "kRaN7CWYTUvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = session.sparkContext"
      ],
      "metadata": {
        "id": "sfM0ngu1TYGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines1 = sc.textFile(\"file1.txt\")  # this is an RDD\n",
        "\n",
        "test1 = lines1.zipWithIndex().flatMap(lambda x: ((i,(x[1],1)) for i in x[0].split(\" \")))\n",
        "test2 = test1.map(lambda x: ((x[0], x[1][0]), x[1][1])).reduceByKey(lambda x, y: x + y)\n",
        "Result_RDD = test2.map(lambda x: (x[0][0], (x[0][1], x[1])))"
      ],
      "metadata": {
        "id": "0bLSMAqXTyLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Result_RDD.take(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S885uUKgTyzX",
        "outputId": "9581f86e-505d-4a92-bb7e-b63324f67ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('against', (302, 1)),\n",
              " ('of', (316, 1)),\n",
              " ('meaning', (459, 1)),\n",
              " ('afterwards', (648, 1))]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines2 = sc.textFile(\"file2.txt\")\n",
        "test1_2 = lines2.zipWithIndex().flatMap(lambda x: ((i,(x[1],1)) for i in x[0].split(\" \")))\n",
        "test2_2 = test1_2.map(lambda x: ((x[0], x[1][0]), x[1][1])).reduceByKey(lambda x, y: x + y)\n",
        "Result_RDD2 = test2_2.map(lambda x: (x[0][0], (x[0][1], x[1])))"
      ],
      "metadata": {
        "id": "2gDqciPyUxP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines3 = sc.textFile(\"file3.txt\")\n",
        "test1_3 = lines3.zipWithIndex().flatMap(lambda x: ((i,(x[1],1)) for i in x[0].split(\" \")))\n",
        "test2_3 = test1_3.map(lambda x: ((x[0], x[1][0]), x[1][1])).reduceByKey(lambda x, y: x + y)\n",
        "Result_RDD3 = test2_3.map(lambda x: (x[0][0], (x[0][1], x[1])))"
      ],
      "metadata": {
        "id": "ebP2SN48hzGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Result_RDD = Result_RDD.union(Result_RDD2).union(Result_RDD3)"
      ],
      "metadata": {
        "id": "AsJX-eprh9J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Result_RDD = Result_RDD.union(Result_RDD2).union(Result_RDD3)\n",
        "\n",
        "Result_RDD.take(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am1yt8l_iBgJ",
        "outputId": "17f04c3d-0190-40f6-ceac-9a82e08ee089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Vespignano,', (269, 1)),\n",
              " ('lay', (304, 1)),\n",
              " ('back', (583, 1)),\n",
              " ('For', (691, 1))]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group words for words.txt according to their first 4 characters and then output the number of members for the first 10 groups."
      ],
      "metadata": {
        "id": "MOHC7EmIAN08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines4 = sc.textFile(\"words.txt\")\n",
        "test1_3 = lines3.zipWithIndex().flatMap(lambda x: ((i,(x[1],1)) for i in x[0].split(\" \")))\n",
        "test2_3 = test1_3.map(lambda x: ((x[0], x[1][0]), x[1][1])).reduceByKey(lambda x, y: x + y)\n",
        "Result_RDD3 = test2_3.map(lambda x: (x[0][0], (x[0][1], x[1])))"
      ],
      "metadata": {
        "id": "KDOP537UAWVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}